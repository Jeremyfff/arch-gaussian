{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import torchvision\n",
    "from model import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    'lansinuote/diffsion_from_scratch.params', safety_checker=None)\n",
    "\n",
    "scheduler = pipeline.scheduler\n",
    "tokenizer = pipeline.tokenizer\n",
    "\n",
    "del pipeline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from datasets import load_dataset\n",
    "#加载数据集\n",
    "dataset = load_dataset(path='lansinuote/diffsion_from_scratch', split='train')\n",
    "\n",
    "\n",
    "#图像增强模块\n",
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    #torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def f(data):\n",
    "    #应用图像增强\n",
    "    pixel_values = [compose(i) for i in data['image']]\n",
    "\n",
    "    #文字编码\n",
    "    input_ids = tokenizer.batch_encode_plus(data['text'],\n",
    "                                            padding='max_length',\n",
    "                                            truncation=True,\n",
    "                                            max_length=77).input_ids\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "dataset = dataset.map(f,\n",
    "                      batched=True,\n",
    "                      batch_size=100,\n",
    "                      num_proc=1,\n",
    "                      remove_columns=['image', 'text'])\n",
    "\n",
    "dataset.set_format(type='torch')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#定义loader\n",
    "def collate_fn(data):\n",
    "    pixel_values = [i['pixel_values'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values).to(device)\n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     batch_size=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#准备训练\n",
    "encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(True)\n",
    "\n",
    "encoder.eval()\n",
    "vae.eval()\n",
    "unet.train()\n",
    "\n",
    "encoder.to(device)\n",
    "vae.to(device)\n",
    "unet.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(),\n",
    "                              lr=1e-5,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "criterion = torch.nn.MSELoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def get_loss(data):\n",
    "    with torch.no_grad():\n",
    "        #文字编码\n",
    "        #[1, 77] -> [1, 77, 768]\n",
    "        out_encoder = encoder(data['input_ids'])\n",
    "\n",
    "        #抽取图像特征图\n",
    "        #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "        out_vae = vae.encoder(data['pixel_values'])\n",
    "        out_vae = vae.sample(out_vae)\n",
    "\n",
    "        #0.18215 = vae.config.scaling_factor\n",
    "        out_vae = out_vae * 0.18215\n",
    "\n",
    "    #随机数,unet的计算目标\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "    #往特征图中添加噪声\n",
    "    #1000 = scheduler.num_train_timesteps\n",
    "    #1 = batch size\n",
    "    noise_step = torch.randint(0, 1000, (1, )).long().to(device)\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    #根据文字信息,把特征图中的噪声计算出来\n",
    "    out_unet = unet(out_vae=out_vae_noise,\n",
    "                    out_encoder=out_encoder,\n",
    "                    time=noise_step)\n",
    "\n",
    "    #计算mse loss\n",
    "    #[1, 4, 64, 64],[1, 4, 64, 64]\n",
    "    return criterion(out_unet, noise)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "def train():\n",
    "    loss_sum = 0\n",
    "    for epoch in range(100):\n",
    "        for i, data in enumerate(tqdm(loader, desc=f'Epoch {epoch}')):\n",
    "            loss = get_loss(data) / 4\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "            if (epoch * len(loader) + i) % 4 == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    #torch.save(unet.to('cpu'), 'saves/unet.model')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "@torch.no_grad()\n",
    "def out_vae_to_image(out_vae):\n",
    "    #从压缩图恢复成图片\n",
    "    #[1, 4, 64, 64] -> [1, 3, 512, 512]\n",
    "    image = vae.decoder(1 / 0.18215 * out_vae)\n",
    "\n",
    "    #转换成图片数据\n",
    "    image = image.cpu()\n",
    "    image = (image + 1) / 2\n",
    "    image = image.clamp(0, 1)\n",
    "    image = image.permute(0, 2, 3, 1)\n",
    "    return image.numpy()[0]\n",
    "\n",
    "def save_image(image, path):\n",
    "    # 将数据重塑为(3, 512, 512)\n",
    "        # reshaped_data = np.transpose(image, (2, 0, 1))\n",
    "\n",
    "        # 将数据缩放到0-255的范围，以便PIL正确显示\n",
    "        reshaped_data = (image * 255).astype(np.uint8)\n",
    "\n",
    "        # 创建一个PIL图像对象\n",
    "        image = Image.fromarray(reshaped_data)\n",
    "\n",
    "        # 显示图像\n",
    "        image.save(path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(text, debug=False):\n",
    "    #词编码\n",
    "    #[1, 77]\n",
    "    pos = tokenizer(text,\n",
    "                    padding='max_length',\n",
    "                    max_length=77,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt').input_ids.to(device)\n",
    "    neg = tokenizer('',\n",
    "                    padding='max_length',\n",
    "                    max_length=77,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "    #[1, 77, 768]\n",
    "    pos = encoder(pos)\n",
    "    neg = encoder(neg)\n",
    "\n",
    "    #[1+1, 77, 768] -> [2, 77, 768]\n",
    "    out_encoder = torch.cat((neg, pos), dim=0)\n",
    "\n",
    "    #vae的压缩图,从随机噪声开始\n",
    "    out_vae = torch.randn(1, 4, 64, 64, device=device)\n",
    "\n",
    "    #生成50个时间步,一般是从980-0\n",
    "    scheduler.set_timesteps(50, device=device)\n",
    "    for time in tqdm(scheduler.timesteps, desc=text):\n",
    "\n",
    "        #往图中加噪音\n",
    "        #[1+1, 4, 64, 64] -> [2, 4, 64, 64]\n",
    "        noise = torch.cat((out_vae, out_vae), dim=0)\n",
    "        noise = scheduler.scale_model_input(noise, time)\n",
    "\n",
    "        #计算噪音\n",
    "        #[2, 4, 64, 64],[2, 77, 768],scala -> [2, 4, 64, 64]\n",
    "        pred_noise = unet(out_vae=noise, out_encoder=out_encoder, time=time)\n",
    "\n",
    "        #从正例图中减去反例图\n",
    "        #[2, 4, 64, 64] -> [1, 4, 64, 64]\n",
    "        pred_noise = pred_noise[0] + 7.5 * (pred_noise[1] - pred_noise[0])\n",
    "\n",
    "\n",
    "        #重新添加噪音,以进行下一步计算\n",
    "        #[1, 4, 64, 64]\n",
    "        out_vae = scheduler.step(pred_noise, time, out_vae).prev_sample\n",
    "\n",
    "        if debug:\n",
    "            image_step = out_vae_to_image(out_vae)\n",
    "            save_path = f\"output/out_vae_{time}.jpg\"\n",
    "            if not os.path.exists(os.path.dirname(save_path)):\n",
    "                os.makedirs(os.path.dirname(save_path))\n",
    "            save_image(image_step, save_path)\n",
    "\n",
    "    #从压缩图恢复成图片\n",
    "    return out_vae_to_image(out_vae)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "image = generate('a city', debug=True)\n",
    "image.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
